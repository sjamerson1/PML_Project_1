---
title: "Practical Machine Learning: Final Project"
author: "Sarah Jamerson"
date: "March 25, 2018"
output: html_document
---

#Overview  

There has been a growing trend for people and their dedication to living a healthy and active lifestyle.  This has brought on an insurgence of devices such as Jawbone Up, Nike FuelBand, and Fitbit, which allows those exercising to keep track of large amounts of information about their physical activities and monitors their health in an inexpensive way.  While the focuses of these devices have been on how much of activities is done, one area of data that is somewhat neglected is how well they do they activity. The Goal of this analysis is to predict the manner in which individuals perform barbell lifts in 5 different ways (correctly and incorrectly).  The following report will cover how the predictive model was built, how cross-validation was utilized, the expected out of sample error. 

#PreProcessing  

Data available here: 

Training data: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv  
Test data: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv  

Install and upload needed packages  

```{r, echo=TRUE}
library(caret)  
library(rpart)  
library(rpart.plot)  
library(RColorBrewer)  
library(randomForest)  
library(ggplot2)
```

Uploading Data:  

```{r}
set.seed(12345)
trainUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"  
testUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"  
training <- read.csv(url(trainUrl), na.strings=c("NA","#DIV/0!",""))  
testing <- read.csv(url(testUrl), na.strings=c("NA","#DIV/0!",""))
```

Then we will divide the training set into two  

```{r}
inTrain <- createDataPartition(training$classe, p=0.6, list=FALSE)
myTraining <- training[inTrain, ]
myTesting <- training[-inTrain, ]
dim(myTraining); dim(myTesting)
```  

#Organizing the Data  

In order to gather numerical data, we must filter out the NearZeroVariance variables and the first column of the training set. We will also remove any variables with a high percentage of NAs (60% or higher).  

Removing the Near ZeroVariance Variables  

```{r}
nzv <- nearZeroVar(myTraining, saveMetrics=TRUE)
myTraining <- myTraining[,nzv$nzv==FALSE]

nzv<- nearZeroVar(myTesting,saveMetrics=TRUE)
myTesting <- myTesting[,nzv$nzv==FALSE]
```  

Removing the first column of the myTraining dataset  

```{r}
myTraining <- myTraining[c(-1)]
```  

Filter out the variables with more than 60% NA  

```{r}
trainingV3 <- myTraining
for(i in 1:length(myTraining)) {
    if( sum( is.na( myTraining[, i] ) ) /nrow(myTraining) >= .7) {
        for(j in 1:length(trainingV3)) {
            if( length( grep(names(myTraining[i]), names(trainingV3)[j]) ) == 1)  {
                trainingV3 <- trainingV3[ , -j]
            }   
        } 
    }
}

# Set back to the original variable name
myTraining <- trainingV3
rm(trainingV3)
``` 

Transform the myTesting and testing datasets  

```{r}
clean1 <- colnames(myTraining)
clean2 <- colnames(myTraining[, -58])  # remove the classe column
myTesting <- myTesting[clean1]         # allow only variables in myTesting that are also in myTraining
testing <- testing[clean2]             # allow only variables in testing that are also in myTraining

dim(myTesting)
``` 

Coerce the data  

```{r}
for (i in 1:length(testing) ) {
    for(j in 1:length(myTraining)) {
        if( length( grep(names(myTraining[i]), names(testing)[j]) ) == 1)  {
            class(testing[j]) <- class(myTraining[i])
        }      
    }      
}

# To get the same class between testing and myTraining
testing <- rbind(myTraining[2, -58] , testing)
testing <- testing[-1,]
``` 

#Model Analysis:

##Analysis One: Decision Tree  

```{r}
set.seed(12345)
modFitA1 <- rpart(classe ~ ., data=myTraining, method="class")
rpart.plot(modFitA1)
```  

Predicition One:  

```{r}
predictionsA1 <- predict(modFitA1, myTesting, type = "class")
cmtree <- confusionMatrix(predictionsA1, myTesting$classe)
cmtree
```  

Accuracy determined by the Decision Tree  

```{r}
plot(cmtree$table, col = cmtree$byClass, main = paste("Decision Tree Confusion Matrix: Accuracy =", round(cmtree$overall['Accuracy'], 4)))
```  

##Analysis Two:; Random Forrests  

```{r}
set.seed(12345)
modFitB1 <- randomForest(classe ~ ., data=myTraining)
predictionB1 <- predict(modFitB1, myTesting, type = "class")
cmrf <- confusionMatrix(predictionB1, myTesting$classe)
cmrf
```  

```{r}
plot(modFitB1)
```  

Random Forest Accuracy  

```{r}
plot(cmrf$table, col = cmtree$byClass, main = paste("Random Forest Confusion Matrix: Accuracy =", round(cmrf$overall['Accuracy'], 4)))
```  

# Prediction Results  

After reviewing the information, it is proven that Random Forests has an accuracy 99.89%, which is higher than that of the Decision Tree which was 87.89%.   The expected out of sample erris is 100.99.89 = 0.11%.  

```{r}
predictionB2 <- predict(modFitB1, testing, type = "class")
predictionB2
```  

